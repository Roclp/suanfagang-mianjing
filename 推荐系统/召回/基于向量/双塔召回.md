双塔模型：
用户塔，物品塔，各输出一个向量，计算cos相似度作为用户兴趣预估值


矩阵补充：
输入：用户id  物品id

双塔模型：
输入：用户id，用户离散特征，用户连续特征  物品id，物品离散特征，物品连续特征
embedding 用户特征拼接  物品特征拼接
分别DNN
得到两个向量
输出：两个向量 进而计算相似度

注意：双塔模型是后期融合（各自DNN后计算相似度）先特征融合再DNN（前期融合）不适用于召回（复杂度高，可用于排序）



Pointwise训练：
把召回看作二分类任务 （用户，物品）
对于正样本，鼓励cos(a,b)接近1
对于负样本，鼓励cos(a,b)接近-1
控制正负样本比例（1:2或1:3）（经验）

Pairwise训练：
模型输入：三元组（用户，正样本，负样本）
基本思想：鼓励cos(a,b) > cos(a,c)
Triple Hinge Loss：max(0, m + cos(a,c) - cos(a,b))
Triple Cross Entropy Loss：-log(sigmoid(cos(a,b) - cos(a,c)))

Listwise训练：
模型输入：用户，一个正样本，多个负样本
基本思想：鼓励cos(a,b)尽量大（接近1），cos(a,c)...cos(a,n)尽量小（接近-1）
softmax + Cross Entropy Loss



双塔模型正负样本选择：
正样本：曝光而且有点击的用户-物品二元组(用户对物品感兴趣)。
问题：少部分物品占据大部分点击，导致正样本大多是热门物品。
解决方案:过采样冷门物品，或降采样热门物品
过采样(up-sampling):一个样本出现多次
降采样(down-sampling):一些样本被抛弃

负样本：

简单负样本：
1. 未被召回的物品
- 未被召回的物品，用户大概率不感兴趣
- 未被召回的物品数量约等于≈全体物品
- 从全体物品中随机抽样，作为负样本
  - 均匀抽样：抽到的负样本大多是冷门物品
  - 所以采用非均匀抽样：打压热门物品    负样本抽样概率和物品流行度成负相关 抽样概率*（点击次数）^(0.75)
  
2. Batch内负样本
   该用户点击其他用户没点
   有偏差，要纠偏


困难负样本：
粗排淘汰的物品（比较困难）  精排分数靠后的物品（比较困难）


对正负样本做二元分类:
- 全体物品(简单)分类准确率高。
- 被粗排淘汰的物品(比较困难)容易分错。
- 精排分数靠后的物品(非常困难)更容易分错。

方法：混合几种负样本
- 50%的负样本是全体物品(简单负样本)
- 50%的负样本是没通过排序的物品(困难负样本)。

曝光但没有点击的样本 不可视为负样本（感兴趣但未点击）    训练召回模型，可以训练排序模型

召回的目标：快速找到用户可能感兴趣的物品（区分不感兴趣和可能感兴趣）
排序的目标：快速找到用户最感兴趣的物品（区分比较感兴趣和非常感兴趣）


双塔模型的线上召回：事先存储物品向量b，线上计算用户向量a
离线存储：用户物品向量b存入向量数据库
在线召回：用户向量查询向量数据库最感兴趣的K个物品
    给定用户id和用户画像，线上计算用户向量a
    最近邻查找，向量a作为query，召回k个物品
why：线上计算代价大，用户兴趣动态变化，而物品特征相对稳定

模型更新：

全量更新：今天凌晨，用昨天全天的数据训练模型
在昨天模型参数的基础上做训练。(不是随机初始化)
用昨天的数据，训练1cpoch，即每天数据只用一遍。
发布新的用户塔神经网络和物品向量，供线上召回使用
全量更新对数据流、系统的要求比较低

增量更新:做 online learning 更新模型参数。
用户兴趣会随时发生变化。
实时收集线上数据，做流式处理，生成TFRecord文件。
对模型做 online learning，增量更新IDEmbedding 参数。(不更新神经网络其他部分的参数。锁住全连接层)
发布用户id Embedding，供用户塔在线上计算用户向量。


能否只做增量更新，不做全量更新？
不行。
小时级数据有偏;分钟级数据偏差更大
全量更新:random shuffle 一天的数据，做1epoch 训练
增量更新:按照数据从早到晚的顺序，做1epoch训练。
随机打乱优于按顺序排列数据，全量训练优于增量训练。




头部效应：
少部分物品占据大部分点击。
大部分物品的点击次数不高。

高点击物品的表征学得好，长尾物品的表征学得不好。

双塔模型+自监督学习：todo
Random Mask
Dropout
互补特征（complementary）
Mask一组关联特征









SENet双塔：来源cv
在双塔模型的feature embedding 层上，引入SENet，对feature embedding 进行特征筛选（学习特征权重）
SENet：Squeeze and Excitation Network
Squeeze：全局池化
Excitation：学习特征权重


Google双塔：双塔最上层进行归一化能提高最终召回效果  而且归一化需要与temperature配套使用
归一化：对user侧和item侧的最后一层输出user embedding和 item embedding，进行L2归一化
温度系数：在归一化之后的向量计算內积之后，除以一个固定的超参 r （0到1） ，论文中命名为温度系数。
解释：
向量点积距离是非度量空间，没有绝对的大小关系，只有相对大小关系（不满足三角不等式）。归一化使其转换为欧式距离。
为什么转欧氏距离：因为ANN一般是通过计算欧式距离进行检索，这样转化成欧式空间，保证训练和检索一致。
只归一化没有温度系数很可能不收敛（logit和交叉熵的关系）：temperature的作用其实就是放大logit，让模型容易学习。


Youtube双塔：最后视为多分类 softmax