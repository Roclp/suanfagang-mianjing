百度-搜索策略部
视频搜索团队（业务、科研均可）
xhs投的

2024-11-05 19：00 一面
55分钟 25分钟问答+25分钟代码（本地ide 归并排序）
提问很基础，但答得不太行

第一个女面试官，主动开摄像头，带了口罩，感觉应该是博士刚毕业，有点点i人的样子
提问按照简历一步一步来，问了很多点，也有一些八股

C++ STL 常用容器：
vector、list、map、set、queue、stack、deque、priority_queue

vector和list的区别：
vector是连续空间，list是链表，所以vector随机访问快，插入删除慢，list插入删除快，随机访问慢
vector的扩容机制：
当vector空间不够用的时候，会扩容，扩容一般是原来空间的1.5倍，如果1.5倍不够用，则直接扩容到原来空间的两倍

map和unordered_map的区别：
map是有序的，unordered_map是无序的，map底层是红黑树，unordered_map底层是哈希表

map的底层实现方式： 红黑树
unordered_map的底层实现方式：哈希表

linux常用命令：
cd ls pwd mkdir rm mv cp cat tar grep find kill ps top killall nvidia-smi
用的还挺多的

用过修改文件权限的命令吗？chmod 怎么用？
chmod怎么用：chmod 777 filename，表示给所有用户读、写、执行权限 

python中list和tuple的区别： list是可变的，tuple是不可变的
lambda函数：匿名函数 没有函数名 只有参数列表和表达式
为什么要用lambda函数：因为有时候需要一个函数，但是又不想给函数起名字，所以用lambda函数

lambda函数的优点：简洁可读，函数式编程
简洁性：Lambda 函数允许你以更简洁的方式定义函数，尤其是当函数体较小且只使用一次时。
匿名性：不需要为函数命名，这在函数只被使用一次的情况下可以减少代码的复杂性。
嵌套定义：Lambda 函数可以在任何地方定义，包括在其他函数内部，这有助于保持代码的模块化和封装性。
函数作为参数：Lambda 函数可以作为参数传递给其他函数，这在需要定义回调函数或实现某些设计模式（如策略模式）时非常有用。
延迟执行：Lambda 函数通常在被调用时才执行，这有助于实现延迟计算（lazy evaluation）。
减少代码量：在某些情况下，使用 Lambda 函数可以减少代码量，使得代码更加简洁和易于理解。
提高代码可读性：对于简单的函数，Lambda 函数可以提高代码的可读性，因为它们可以直观地表达函数的意图。
函数式编程：Lambda 函数是函数式编程范式的一部分，它们支持高阶函数和函数组合，使得代码更加灵活和可重用。
并发和多线程：在多线程或并发编程中，Lambda 函数可以简化线程的创建和管理。

介绍mapReduce：分治思想
MapReduce的原理：Map阶段将数据分片，Reduce阶段将数据汇总
可以输入多个文件吗？可以，多个文件会分别进行MapReduce，最后汇总

什么时候用hadoop：大数据处理，数据量很大，无法在单机上处理，数据存储在集群的多个节点上

深度遍历和层序遍历：栈和队列
深度遍历：递归，先访问根节点，然后访问左子树，再访问右子树 
层序遍历：广度优先搜索，先访问根节点，然后访问下一层的节点，再访问下一层的节点，直到访问完所有节点

介绍快速排序：
快速排序的原理：选择一个基准元素，将数组分为两部分，一部分小于基准元素，一部分大于基准元素，然后递归地对这两部分进行快速排序
快速排序的时间复杂度：平均情况下是O(nlogn)，最坏情况下是O(n^2)
快速排序的空间复杂度：O(logn)
快速排序的稳定性：不稳定

介绍逻辑回归：
逻辑回归的原理：通过线性回归得到一个预测值，然后通过sigmoid函数将预测值映射到0-1之间，表示样本属于正类的概率
逻辑回归的损失函数：交叉熵损失函数
逻辑回归的优化方法：梯度下降法
逻辑回归的适用场景：二分类问题

逻辑回归和softmax的区别：
逻辑回归只能用于二分类问题，softmax可以用于多分类问题
逻辑回归的输出是0-1之间的概率，softmax的输出是每个类别的概率

常用的损失函数：
均方误差损失函数：用于回归问题
交叉熵损失函数：用于分类问题
KL散度损失函数：用于度量两个概率分布之间的差异
Hinge损失函数：用于支持向量机（SVM）问题
对数损失函数：用于逻辑回归问题
Huber损失函数：用于回归问题，在均方误差损失函数和绝对误差损失函数之间进行平滑过渡

介绍k-means算法：
k-means算法的原理：将数据分为k个簇，每个簇的中心点为该簇的均值，通过迭代更新簇的中心点和簇的成员，直到收敛
k-means算法的时间复杂度：O(tkn)，其中t是迭代次数，k是簇的数量，n是样本的数量
k-means算法的空间复杂度：O(nk)，其中n是样本的数量，k是簇的数量
k-means算法的稳定性：不稳定，容易受到初始簇中心点的选择影响

停止条件：当簇的中心点不再发生变化或者达到最大迭代次数时停止。

如何确定k值：肘部法则、轮廓系数、Gap Statistic等
肘部法则：通过绘制不同k值下的簇的数量和簇的均方差，找到肘部，肘部的k值即为最优的k值
轮廓系数：通过计算每个样本的轮廓系数，取平均轮廓系数最大的k值
Gap Statistic：通过计算不同k值下的Gap Statistic，取Gap Statistic最大的k值

介绍transformer模型：
transformer模型是一种基于自注意力机制的深度神经网络模型，用于处理序列数据，如文本、语音等
transformer模型的原理：通过自注意力机制，将输入序列中的每个元素与输入序列中的其他元素进行交互，得到每个元素的表示，然后通过全连接层和激活函数得到最终的输出

没学过nlp？是的 不太了解

介绍BN：
介绍LN：
BN和LN的区别：BN是在batch维度上做归一化，LN是在feature维度上做归一化
哪一个是对相同样本（单一样本）进行归一化：LN （当时没答上来）

L1范数和L2范数：L1范数是绝对值的和，L2范数是平方和的平方根
L0范数是指向量中非0的元素的个数。(L0范数很难优化求解)

L1范数是指向量中各个元素绝对值之和

L2范数是指向量各元素的平方和然后求平方根

L1范数可以进行特征选择，即让特征的系数变为0.

L2范数可以防止过拟合，提升模型的泛化能力，有助于处理 condition number不好下的矩阵(数据变化很小矩阵求解后结果变化很大)

（核心：L2对大数，对outlier离群点更敏感！）

下降速度：最小化权值参数L1比L2变化的快

模型空间的限制：L1会产生稀疏 L2不会。

L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。

两个项目

代码：本地ide 归并排序
搞了25分钟，终于搞出来的，空间有点浪费（现想出来的 哈哈 有好几处小bug 面试官有指点 我也发现了一些 最后应该是改对了）

反问：
1. 搜广推和其他算法的就业情况：差不多，算法都是通用的，还是看具体业务，但是不同公司不同部门业务也是不一样的 大搜 Paddle
2. 还有后续面试机会吗？还有等HR确定，似乎面试官也没法确定 研究生导师有安排吗


2024-11-06 16：30
HR电话通知一面过了，二面约到2024-11-08 19：00

二面 一小时 没开摄像头，发现如流面试也可以在线做题 ACM模式