梯度消失的原因：
1. 激活函数：Sigmoid、Tanh等激活函数在输入值较大或较小时，导数接近0，导致梯度消失。
2. 网络深度：网络层数过多，导致梯度在反向传播过程中逐渐变小，导致梯度消失。
3. 学习率：学习率过大，导致梯度在反向传播过程中逐渐变大，导致梯度爆炸。

梯度爆炸的原因：
1. 激活函数：ReLU、Leaky ReLU等激活函数在输入值较大时，导数接近1，导致梯度爆炸。
2. 网络深度：网络层数过多，导致梯度在反向传播过程中逐渐变大，导致梯度爆炸。

缓解梯度消失的方法：
1. 激活函数：使用ReLU、Leaky ReLU、ELU等激活函数
2. 正则化：L1、L2、Dropout
3. 批标准化：Batch Normalization
4. 梯度裁剪：Gradient Clipping

缓解梯度爆炸的方法：
1. 正则化：L1、L2、Dropout
2. 批标准化：Batch Normalization
3. 梯度裁剪：Gradient Clipping
4. 使用RMSprop、Adam等优化器

缓解过拟合的方法：
1. 正则化：L1、L2、Dropout
2. 批标准化：Batch Normalization
3. 数据增强：Data Augmentation
4. Dropout
5. Early Stopping
6. 使用更小的网络
7. 使用更少的数据

缓解欠拟合的方法：
1. 增加网络容量：使用更大的网络、更多的层、更多的神经元
2. 使用更多的数据
3. 使用更复杂的模型
4. 调整学习率：使用学习率衰减、学习率预热
5. 使用正则化：L1、L2权重衰减

L1、L2正则化：L1正则化会使得权重稀疏，L2正则化会使得权重变小，但不会稀疏。
为什么L1正则化会使得权重稀疏，L2正则化会使得权重变小，但不会稀疏？
1. L1正则化：L1正则化会在损失函数中添加权重绝对值的和，使得权重在训练过程中逐渐变小，最终变为0，从而实现稀疏。(解空间是菱形，使得一些参数变为0，稀疏解可进行特征选择)Lasso回归（Lasso回归）
2. L2正则化：L2正则化会在损失函数中添加权重平方的和，使得权重在训练过程中逐渐变小，但不会变为0，从而实现权重变小。（解空间是球形，防止参数过大，使模型稳定）Ridge回归（岭回归）