循环神经网络(Recurrent Neural Network，RNN)是一类能够处理序列数据的神经网络，它特别擅长处理时间序列数据或序列数据，比如文本、语音、视频等。RNN 的特点是它们具有隐层状态，这种状态能够存储前一步的信息，并在当前时刻的计算中使用，使得网络能够保留历史信息。
RNN的主要特点:
1)席列处理能力:RNN 能够处理任意长度的序列输入，因为它们能够逐步处理输入序列的每个元素，并将当前的状态传递到下一个时间步。
2)隐状态传播:每个时间步的隐状态是当前输入和前一隐状态的函数，这使得 RNN 能够保留过去的信息,3)共享权重:RNN 在每个时间步上使用相同的权重，这种权重共享的特性使得 RNN 能够处理变长序列。
RNN的局限性:
1)梯度消失和梯度爆炸问题:由于 RNN 在训练过程中通过链式法则更新参数，随着时间步的增加，梯度会指数级增长或衰减，导致梯度爆炸或梯度消失，从而影响长序列的学习。2)长距离依赖问题:普通 RNN 很难捕捉和记住长距离的依赖关系，导致对于长序列中的远距离信息处理效果不生。
3)计算效率低:由于 RNN 需要逐个时间步进行计算，这样使得训练过程较慢，相较于并行计算的其他网络结构差距较大。


RNN 虽然有以上局限性，但在序列数据处理方面仍发挥了重要作用。为了改善 RNN 的不足，人们提出了几种改进方法和变体，以下是几个主要的:
1)长短期记忆网络(LSTM):LSTM 是一种特殊的 RNN 结构，通过引入"门”(如输入门、遗忘门、输出门)来更好地保存长期信息，能够解决梯度消失和梯度爆炸的问题，擅长捕捉长距离依赖。
2)门控循环单元(GRU):GRU 是 LSTM 的一种简化版本，它将 LSTM 的多个门机制合并成两个门(重置门和更新门)，从而降低了计算复杂度，同时在某些应用中效果不逊色于 LSTM。
3)双向 RNN(Bi-RNN):双向 RNN 通过在每个时间步同时考虑前向和后向的信息，可以捕捉到更丰富的语境信息，增强网络的建模能力。