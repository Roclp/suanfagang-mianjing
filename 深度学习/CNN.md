CNN的主要组成部分：
1. 卷积层（Convolutional Layer）：负责提取输入数据的特征。通过使用卷积核（过滤器）来扫描输入图片，以提取局部特征。
2. 激活函数（Activation Function）：通常使用ReLU（Rectified Linear Unit），负责引入非线性，可以使得模型表达更加复杂的特征。
3. 池化层（Pooling Layer）：通过对邻近区域进行降采样，减少数据量及参数数目，从而降低计算复杂度，同时减小模型的过拟合风险。常见的池化方法有最大池化（MaxPooling）和平均池化（Average Pooling）。
4. 全连接层（Fully Connected Layer）：将之前层所得的特征进行整合，最后通过Softmax等激活函数实现分类或回归任务。

卷积层的基本参数：
1. 卷积核（Kernel）：也称为过滤器（Filter），是进行卷积操作的核心部分，通常是一个二维的矩阵。
2. 步幅（Stride）：步幅决定了卷积核在输入特征图上滑动的步长。
3. 填充（Padding）：填充是在输入数据的边界添加额外的像素以控制输出特征图的尺寸。
4. 激活函数（Activation Function）：在卷积操作之后应用的非线性变换，用于引入非线性特征使神经网络能够准确地拟合复杂的数据模式。

如何理解卷积层的深度？
卷积层的深度通常指的是卷积核的数量，即每个卷积层可以包含多个卷积核，每个卷积核都会生成一个特征图。这些特征图在后续的层中可以组合起来，提取更复杂的特征。因此，卷积层的深度可以理解为特征图的通道数，它决定了网络能够提取多少种不同的特征。

卷积核大小、步幅和填充对输出特征图尺寸的影响：
卷积核大小F，步长S，填充P
H'=(H-F+2P)/S+1, W'=(W-F+2P)/S+1

1×1卷积 旨在对每个空间位置的D维向量做一个相同的线性变换。通常用于增加非线性，或降维，这相当于在通道数方向上进行了压缩。1×1卷积是减少网络计算量和参数的重要方式。
1x1卷积在卷积神经网络(CNN)中有几个非常重要的作用:
1)降维和升维:1x1卷积能够改变输入特征图的通道数，从而起到降维(减少通道数)或升维(增加通道数)的作用。这对于降低计算量和模型参数，提升模型效率非常有帮助。
2)跨通道的信息交互:1x1卷积可以通过每个像素点在不同通道上的卷积操作，实现通道之间的信息融合和交互，从而增强特征表达能力。
3)加入非线性:1x1卷积通常与非线性激活函数(如ReLU)结合，推动模型学习到更复杂的非线性特征。
4)特征重组:作为网络的构建块，1x1卷积可以重新组织特征，使得特征更适合接下来的处理步骤。例如在一些残差网络(ResNet)中，1x1卷积可以配合其它尺寸的卷积层完成特征的调整和组合。
扩展知识
1x1 卷积的作用可以从以下几个方面进一步理解:
1)网络加速:在大型模型如Inception网络中，使用1x1卷积可以大幅减少计算量，从而加速网络训练和推理。其背后的原理是在不损失特征图宽高的情况下，通过减少通道数来降低计算复杂度。
2)唯一性映射:1x1卷积核相当于对每个像素点进行一维向量变换，因此能够辅助实现唯一性映射，使模型能够捕捉到更多元的特征。
3)作用于深层卷积网络:在ResNet等深层模型中，1x1卷积常被用于特征对齐、通道重组与特征重组，使深层网络更加灵活和高效。
4)卷积解释性:由于其小尺寸，1x1卷积有助于理解每个通道的单独贡献，这在解释性研究中尤为有用。
5)实际应用:1x1卷积被广泛应用于各类深度学习任务中，例如图像分类、目标检测、语义分割等领域，都能通过1x1卷积实现高效的特征提取和模型优化。


https://zhuanlan.zhihu.com/p/31727402