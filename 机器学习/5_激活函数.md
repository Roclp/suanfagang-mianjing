# 激活函数的作用

引入非线性因素，使得神经网络可以逼近任何非线性函数

# 常见的三种激活函数

|          |                           sigmoid                            |                          tanh                          |                             ReLU                             |
| :------: | :----------------------------------------------------------: | :----------------------------------------------------: | :----------------------------------------------------------: |
|   公式   |                  $f(x)=\frac{1}{1+e^{-x}}$                   |          $f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$          |                       $f(x)=max(0,x)$                        |
|   导数   |                     $f'(x)=f(x)(1-f(x))$                     |                    $f'(x)=1-f^2(x)$                    |       $f'(x)=\begin{cases}1,x>0\\0,x\leq0\end{cases}$        |
| 梯度消失 |                           容易造成                           |               也容易造成，但优于sigmoid                |                     可以减缓，优于前两者                     |
| 常见应用 |                          二分类任务                          |                      **RNN网络**                       |                           CNN网络                            |
|   优点   |                      函数平滑，容易求导                      |        ①函数平滑，容易求导<br>②输出关于零点对称        | ①求导更快，收敛更快   <br>②有效缓解了梯度消失问题<br>③增加网络的稀疏性 |
|   缺点   | ①容易造成梯度消失       <br>②存在幂运算，计算量大<br>③其输出不关于零点对称 |     ①容易造成梯度消失  <br>②同样存在计算量大的问题     |                    容易造成神经元的“死亡”                    |
|   图形   |    ![](https://i.loli.net/2020/06/08/HIX7TKyU2MsqlbV.png)    | ![](https://i.loli.net/2020/06/08/9DEFnfop1qmNM7T.png) |    ![](https://i.loli.net/2020/06/08/Nc2aBh3O5pEdk4Y.png)    |


# 参考资料

[常见激活函数总结（sigmoid、Tanh、ReLU等）及激活函数面试常见问题总结](https://blog.csdn.net/neo_lcx/article/details/100122938)