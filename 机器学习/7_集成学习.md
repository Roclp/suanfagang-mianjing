集成学习：**好而不同**，训练多个分类器，然后通过投票或者加权平均的方式得到最终结果

# bagging套袋法

![bagging](https://i.loli.net/2020/05/25/6HXMElOCfYVqFha.png)

算法过程：

从原始数据集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

# boosting(提升方法)

![boosting](https://i.loli.net/2020/05/25/7DzdFVx8GasNHRo.png)

AdaBoost算法（Adaptive Boosting）：
算法流程：
1. 初始化样本权重：给每个训练样本分配一个初始权重，通常是均匀分布的。
2. 训练弱分类器：根据当前样本权重，训练一个弱分类器。
3. 计算分类误差：根据弱分类器的预测结果，计算分类误差。
4. 更新样本权重：根据分类误差调整样本权重，使得分类错误的样本权重增加，分类正确的样本权重减少。
5. 更新分类器权重：根据分类误差计算弱分类器的权重，使得分类表现好的分类器权重更高。
6. 迭代上述步骤，直到达到预定的弱分类器数量或错误率达到某个阈值。
7. 将所有弱分类器进行加权求和，得到最终的分类结果。

AdaBoost算法优点：
1. 自动学习：Adaboost 能够自动关注那些难以分类的样本，并逐渐提高它们在分类器中的重要性。
2. 简单且有效：该算法结构简单，易于实现，但在很多实际应用中非常有效。

AdaBoost算法缺点：
1. 对噪声和异常值非常敏感：Adaboost算法会对误分类的样本赋予更高的权重，错误分类的样本权重在后续过程中会被放大，可能导致整体结果受到不良影响。
如何减少对噪声和异常值的敏感性？
1. Data Preprocessing:对数据进行预处理，移除或修正明显的噪声。
2. Robust Variants: 使用一些对噪声更具鲁棒性的 Adaboost 变体，比如 Gentle Adaboost 或者 RobustAdaboost.
3. Regularization:引|入正则化项，以减少模型的复杂度，防止过拟合。


算法过程：

初始化训练数据集，给每条训练数据一个权重，可以认为是每个样本的权重向量，初始时，每个样本的权重都相等。

前一轮分类错误的样本，得到更高的权重，即通过迭代算法，每一轮都根据上一轮的分类结果，调整样本的权重，使得上一轮分类错误的样本在下一轮得到更多的关注。

AdaBoot和Random Forest的区别：
https://www.mianshiya.com/bank/1821834636175642625/question/1821834651967197185


GBDT（Gradient Boosting Decision Tree 梯度提升决策树）：
算法流程：
1. 初始化弱分类器：选择一个初始的弱分类器，例如一个简单的决策树。
2. 计算残差：计算每个样本的残差，即实际值与预测值的差值。
3. 训练新的弱分类器（拟合残差）：使用残差作为目标，训练一个新的弱分类器。
4. 更新模型：将新的弱分类器加入到模型中，模型更新为现有模型加上新树的预测结果。
5. 重复步骤2-4，直到达到预设的迭代次数或预设的停止条件，比如残差减少到某个水平。

GBDT的常见实现方式：
1. XGBoost：XGBoost 它在训练过程中使用了多种优化技术，如树剪枝、正则化等，以提高模型的性能和效率。
2. LightGBM：LightGBM 它采用了许多优化技术，如直方图算法、叶子节点分裂策略等，以提高模型的性能和效率。
3. CatBoost：CatBoost 它特别适用于处理类别型特征，通过将类别型特征转换为数值型特征，提高了模型的性能和效率。

GB（Gradient Boosting 梯度提升）是一种方法，XGBoost（eXtreme Gradient Boosting）是GB的一种实现，它是一种基于决策树的集成学习方法。


# bagging和boosting的区别
①样本选择上：

Bagging：训练集是在原始集中有放回随机选取的，各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

②样例权重：

Bagging：使用均匀取样，每个样例的权重相等。

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

③预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

④并行计算：

Bagging：各个预测函数可以并行生成。

Boosting：各个预测函数必须顺序生成，因为后一个训练集的生成需要前一个训练集的结果。


# stacking模型融合

stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。

![stacking](https://i.loli.net/2020/05/25/DQZo5tOAhq2uKBl.png)


# 参考资料

[Boosting原理](https://www.jianshu.com/p/11083abc5738)

[提升方法(boosting)详解](https://www.cnblogs.com/linyuanzhou/p/5019166.html)

[Bagging和Boosting的区别](https://www.cnblogs.com/earendil/p/8872001.html)

[Stacking算法](https://www.jianshu.com/p/59313f43916f)



https://zhuanlan.zhihu.com/p/148050748#:~:text=boosting%20%E2%80%A2%E4%B8%B2%E8%A1%8C%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%9F%BA%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%8C%E5%90%84%E5%88%86%E7%B1%BB%E5%99%A8%E4%B9%8B%E9%97%B4%E6%9C%89%E4%BE%9D%E8%B5%96%E3%80%82%E6%AF%8F%E6%AC%A1%E8%AE%AD%E7%BB%83%E6%97%B6%EF%BC%8C%E5%AF%B9%E5%89%8D%E4%B8%80%E5%B1%82%E5%9F%BA