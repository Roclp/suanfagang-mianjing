常用的激活函数：
sigmoid：将输入值映射到0-1之间，常用于二分类问题
tanh：将输入值映射到-1-1之间，常用于二分类问题
relu：修正线性单元，将小于0的值置为0，大于0的值保持不变，常用于多层神经网络
softmax：将输入值映射到0-1之间，并且所有输出值的和为1，常用于多分类问题
leaky relu：修正线性单元，将小于0的值置为0，大于0的值保持不变，但小于0的值会乘以一个小于1的系数，常用于多层神经网络
elu：指数线性单元，将小于0的值置为0，大于0的值保持不变，但小于0的值会乘以一个指数函数，常用于多层神经网络
swish：将输入值乘以一个sigmoid函数，常用于多层神经网络


Sigmoid 函数的性质
平滑且连续:Sigmoid 函数光滑且可微，这意味着我们可以对其进行梯度下降优化。
单调递增:随着输入的增加，输出值逐渐升高，但永远不到达0或1。
对称性:Siqmoid 函数关于原点对称，并且在(x=0,y=0.5)处有一个转折点。
逻辑回归中的具体使用:
模型构建:逻辑回归首先通过线性回归模型得到一个线性组合值Z=WX+b。
概率映射:将这个Z值通过 Siqmoid 函数映射到 [0,1] 区间，得到 P(y=1|X)= σ(Z)。
优化目标:通过最大化似然函数或者最小化二元交叉熵损失来优化逻辑回归模型的参数。
与其他激活函数比较:
Tanh 函数:Tanh 也是常用的激活函数，输出范围在 -1 到1之间，能够处理数据的中心化,常用于神经网
络。ReLU 函数:ReLU(Rectified linear Unit)函数在神经网络中使用广泛，处理非线性问题时有更好的梯度传递性能，但不常用于概率模型如逻辑回归中。
可能存在的缺点:
梯度弥散问题:Sigmoid 函数在极端值上的梯度接近于零，可能会导致梯度弥散，影响深层神经网络的训练效果。
非零中心化:Sigmoid 函数的输出不是零中心化的，当输入值为负时，输出值不会非常接近零，这可能导致收敛速度较慢。