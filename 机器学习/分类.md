逻辑回归：


SVM：
找到一个超平面，也就是支持向量，使得类别之间的间隔最大，也就是不同类的点

拉格朗日对偶优化问题：

硬间隔：
软间隔：
松弛变量：
核函数：线性核、多项式核、高斯核（RBF）、Sigmoid核


决策树：
算法流程：
1. 对训练集进行特征选择
2. 根据选择的特征将训练集划分为不同的子集
3. 对每个子集递归地调用步骤1和步骤2，直到满足停止条件
4. 达到预设停止条件（最大深度、最小样本数、信息增益小于阈值等）时停止递归，将当前节点标记为叶节点，并将该节点的所有样本分配到对应的类别标签

优化：
剪枝：防止过拟合 预剪枝（在决策树生成过程中进行剪枝）和后剪枝（在决策树生成完成后进行剪枝）
缺失值处理：填补（均值、中位数、众数）、删除（跳过）、插值（k近邻）
连续值处理：连续特征离散化（分箱、二值化）

非数值型特征怎么处理：类别编码、向量编码（one-hot encoding、label encoding）

决策树的优点：
1. 简单易懂：决策树算法直观易懂，易于理解和解释。
2. 可解释性强：决策树的结构和规则可以直观地展示出来，便于理解和解释。

决策树的缺点：
1. 过拟合：决策树容易过拟合，特别是在数据量较小或特征较多的情况下（可以通过剪枝、设置最大深度等方法进行优化）。
2. 对噪声敏感：决策树对噪声和异常值敏感，可能会受到噪声和异常值的影响，导致模型性能下降。

决策数解决欠拟合：
1. 增加树的深度（max_depth）让模型更深
2. 减少最小样本分裂数（min_samples_split）让模型更容易分裂
3. 减少最小叶子节点数（min_samples_leaf）让模型更容易成为叶子节点

决策树解决过拟合：
1. 限制树的深度（max_depth）让模型更浅
2. 增加最小样本分裂数（min_samples_split）让模型更不容易分裂    
3. 增加最小叶子节点数（min_samples_leaf）让模型更不容易成为叶子节点
4. 剪枝：在决策树生成过程中进行剪枝，防止过拟合

CART决策树：
回归树：
分类树：

随机森林：决策树Bagging
算法流程：
1. 样本抽样：从原始训练集中通过有放回的抽样(即Bootstrap抽样)获得多个子训练集。
2. 特征选择：在每个节点进行分裂时，从所有特征中随机选择一个特征子集，然后选择最优特征进行分裂。
3. 树的生成：使用每个子训练集来训练不同的决策树(不剪枝)。
4. 投票机制：在预测阶段，综合所有决策树的预测结果(分类任务采用多数投票，回归任务取平均值)。

如何理解随机森林的随机性：
1. 样本选择随机性：随机森林通过有放回的抽样(即Bootstrap抽样)来获得多个子训练集，这引入了随机性。
2. 特征选择随机性：在每个节点进行分裂时，随机选择一个特征子集，这引入了随机性。



KNN：
算法流程：
1. 计算已知类别数据集中的点与当前点之间的距离
2. 按照距离递增次序排序
3. 选择与当前点距离最小的k个点
4. 确定前k个点所在类别的出现频率
5. 返回前k个点出现频率最高的类别作为当前点的预测分类

如何确定距离：
欧氏距离、曼哈顿距离、切比雪夫距离、明可夫斯基距离、汉明距离、杰卡德相似系数、余弦相似度、皮尔逊相关系数

如何选择K值：
交叉验证：
尽量选奇数K，防止出现平局。

K值过大会欠拟合：问了太多并不了解你的人
K值过小会过拟合：只问了几个非常了解你的人

KNN算法技巧：
归一化：基于距离计算，防止某些取值范围大的特征对距离计算产生不适当的影响，确保所有特征对距离计算的贡献是“公平”的。
加权投票：距离越近，权重越大。

KNN计算复杂度高，特征维度高时，如何优化：
降维：PCA、t-SNE、LDA

KNN优点：
1. 简单易懂：KNN 算法简单直观，易于理解和实现。
2. 无需训练：KNN 算法不需要显式的训练过程，它是一个惰性学习方法，只需保存训练数据即可。
3. 灵活性好：适用于分类和回归问题，且能有效处理多类分类问题。
4. 参数少：除了K的取值和距离度量方式外，无需调整过多的超参数。

KNN缺点：
1. 计算复杂度高：每次预测都需要计算样本与所有训练数据的距离，时间复杂度高，尤其在大规模数据集上表现不佳。
2. 存储需求大：需要存储所有训练数据，空间复杂度高。
3. 对异常值敏感：KNN 算法对噪声和离群点敏感，可能会受到异常值的影响。

朴素贝叶斯：

