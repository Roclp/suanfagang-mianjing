常见的损失函数：
交叉熵：-ylog(y')-(1-y)log(1-y')
均方误差：(y-y')^2
均方根误差：sqrt((y-y')^2)
平均绝对误差：|y-y'|
对数损失函数：-ylog(y')-(1-y)log(1-y')
合页损失函数：max(0,1-y*y')
铰链损失函数：max(0,1-y*y')
指数损失函数：exp(-y*y')
感知损失函数：max(0,1-y*y')


为什么分类算法要使用交叉熵损失函数，而不使用均方误差损失函数？
1. 概率解释：交叉熵是信息论中的概念，用来衡量两个概率分布之间的相似度，而分类问题的输出是概率分布，所以使用交叉熵损失函数更合适。
2. 梯度解释：交叉熵损失函数对于参数的调整更加敏感，可以加快模型的收敛速度。
